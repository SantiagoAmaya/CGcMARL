"""
logger.py
==========
Unified, optional logging for console ★, Weights & Biases, or TensorBoard.

What it does
------------
`setup_logger()` returns the pair

    log_fn(metrics: Mapping[str, Any], step: int | None) -> None
    close_fn() -> None

where `log_fn` records metrics to the chosen backend *and* echoes a concise
`k=v` string to stdout so you always have human-readable feedback, even on a
headless cluster.

Back-ends
---------
* **console**  (default fallback) – standard `logging` to stdout.  
* **wandb**    (if the *wandb* package is importable).  
* **tensorboard** (*torch.utils.tensorboard.SummaryWriter*).

File relations
--------------
* `train.py` calls `setup_logger(...)` once at start-up and keeps the returned
  `log_fn` / `close_fn` in scope.  
* Other modules **never** import `logger` directly—they receive the callable
  through dependency injection, which keeps testing simple.

Design notes
------------
* Zero required extras: if a backend is missing, the code gracefully falls back
  to pure console logging.  
* Keeps the public surface tiny—one function, two callables—to avoid vendor
  lock-in and speed up refactors.

"""


from __future__ import annotations

import logging
import atexit
from datetime import datetime
from typing import Any, Mapping 
from numbers import Number


def _fmt(metrics: Mapping[str, Any]) -> str:
    """Return stable `k=v` string tolerant to non-numeric values."""
    return " ".join(
        f"{k}={v:.4g}" if isinstance(v, Number) else f"{k}={v}"
        for k, v in metrics.items()
    )


def setup_logger(
    backend: str = "console",  # "wandb", "tensorboard", "tb", "console"
    *,
    project: str = "three_layer_pd_marl",
    run_id: str | None = None,
    **wandb_kwargs,
):
    """Return `(log_fn, close_fn)` suitable for the training loop.

    Parameters
    ----------
    backend : {"wandb", "tensorboard", "tb","console"}
        Preferred logging backend.  Falls back to console if unavailable.
    project : str
        WandB / TensorBoard project name.
    run_id : str | None
        Optional unique run identifier; autogenerated if omitted.
    wandb_kwargs : Any
        Extra keyword arguments forwarded to `wandb.init`.
    """
    backend = backend.lower()

    # ---------------------------------------------------------------------
    # Configure root console logger once (idempotent)
    # ---------------------------------------------------------------------
    logger = logging.getLogger(project)
    logger.propagate = False 
    if not logger.handlers:
        logger.setLevel(logging.INFO)
        logger.addHandler(logging.StreamHandler())

    # ---------------------------------------------------------------------
    # Weights & Biases backend
    # ---------------------------------------------------------------------
    if backend == "wandb":
        try:
            import wandb  # type: ignore
        except ModuleNotFoundError:
            logger.warning("wandb not found → falling back to console logging")
        else:
            run = wandb.init(project=project, id=run_id, resume="allow", **wandb_kwargs)

            def _log(metrics: Mapping[str, Any], step: int | None = None):
                wandb.log(metrics, step=step)
                msg = _fmt(metrics)
                if step is not None:
                    msg = f"[step {step}] {msg}"
                logger.info(msg)

            def _close():
                run.finish()
            atexit.register(_close)

            return _log, _close

    # ---------------------------------------------------------------------
    # TensorBoard backend
    # ---------------------------------------------------------------------
    if backend in {"tb", "tensorboard"}:
        try:
            from torch.utils.tensorboard import SummaryWriter  # pylint: disable=import-error
        except ModuleNotFoundError:
            logger.warning("TensorBoard not available → falling back to console logging")
        else:
            log_dir = f"runs/{project}/{run_id or datetime.now().strftime('%Y%m%d-%H%M%S')}"
            writer = SummaryWriter(log_dir)

            def _log(metrics: Mapping[str, Any], step: int | None = None):
                for k, v in metrics.items():
                    if isinstance(v, Number):
                        # Cast to native so TB protobuf doesn't choke on numpy types
                        writer.add_scalar(k, float(v), step)
                msg = _fmt(metrics)
                if step is not None:
                    msg = f"[step {step}] {msg}"
                logger.info(msg)

            def _close():
                writer.flush()
                writer.close()
            atexit.register(_close)

            return _log, _close

    # ---------------------------------------------------------------------
    # Fallback – console only
    # ---------------------------------------------------------------------
    def _log(metrics: Mapping[str, Any], step: int | None = None):  # noqa: D401
        msg = _fmt(metrics)
        if step is not None:
            msg = f"[step {step}] {msg}"
        logger.info(msg)

    def _close():
        pass
    atexit.register(_close)

    return _log, _close
