"""
logger.py
==========
Unified, optional logging for console ★, Weights & Biases, or TensorBoard.

What it does
------------
`setup_logger()` returns the pair

    log_fn(metrics: Dict[str, float], step: int | None) -> None
    close_fn() -> None

where `log_fn` records metrics to the chosen backend *and* echoes a concise
`k=v` string to stdout so you always have human-readable feedback, even on a
headless cluster.

Back-ends
---------
* **console**  (default fallback) – standard `logging` to stdout.  
* **wandb**    (if the *wandb* package is importable).  
* **tensorboard** (*torch.utils.tensorboard.SummaryWriter*).

File relations
--------------
* `train.py` calls `setup_logger(...)` once at start-up and keeps the returned
  `log_fn` / `close_fn` in scope.  
* Other modules **never** import `logger` directly—they receive the callable
  through dependency injection, which keeps testing simple.

Design notes
------------
* Zero required extras: if a backend is missing, the code gracefully falls back
  to pure console logging.  
* Keeps the public surface tiny—one function, two callables—to avoid vendor
  lock-in and speed up refactors.

"""


from __future__ import annotations

import logging
from datetime import datetime
from typing import Dict, Optional


def _fmt(metrics: Dict[str, float]) -> str:
    """Compact "k=v ..." string for console printing"""
    return " ".join(f"{k}={v:.4f}" for k, v in metrics.items())


def setup_logger(
    backend: str = "wandb",
    *,
    project: str = "three_layer_pd_marl",
    run_id: str | None = None,
    **wandb_kwargs,
):
    """Return `(log_fn, close_fn)` suitable for the training loop.

    Parameters
    ----------
    backend : {"wandb", "tensorboard", "console"}
        Preferred logging backend.  Falls back to console if unavailable.
    project : str
        WandB / TensorBoard project name.
    run_id : str | None
        Optional unique run identifier; autogenerated if omitted.
    wandb_kwargs : Any
        Extra keyword arguments forwarded to `wandb.init`.
    """
    backend = backend.lower()

    # ---------------------------------------------------------------------
    # Configure root console logger once (idempotent)
    # ---------------------------------------------------------------------
    logger = logging.getLogger(project)
    if not logger.handlers:
        logger.setLevel(logging.INFO)
        logger.addHandler(logging.StreamHandler())

    # ---------------------------------------------------------------------
    # Weights & Biases backend
    # ---------------------------------------------------------------------
    if backend == "wandb":
        try:
            import wandb  # type: ignore
        except ModuleNotFoundError:
            logger.warning("wandb not found → falling back to console logging")
        else:
            run = wandb.init(project=project, id=run_id, resume="allow", **wandb_kwargs)

            def _log(metrics: Dict[str, float], step: int | None = None):
                wandb.log(metrics, step=step)
                logger.info(_fmt(metrics))

            def _close():
                wandb.finish()

            return _log, _close

    # ---------------------------------------------------------------------
    # TensorBoard backend
    # ---------------------------------------------------------------------
    if backend in {"tb", "tensorboard"}:
        try:
            from torch.utils.tensorboard import SummaryWriter  # pylint: disable=import-error
        except ModuleNotFoundError:
            logger.warning("TensorBoard not available → falling back to console logging")
        else:
            log_dir = f"runs/{project}/{run_id or datetime.now().strftime('%Y%m%d-%H%M%S')}"
            writer = SummaryWriter(log_dir)

            def _log(metrics: Dict[str, float], step: int | None = None):
                for k, v in metrics.items():
                    writer.add_scalar(k, v, step)
                logger.info(_fmt(metrics))

            def _close():
                writer.flush()
                writer.close()

            return _log, _close

    # ---------------------------------------------------------------------
    # Fallback – console only
    # ---------------------------------------------------------------------
    def _log(metrics: Dict[str, float], step: int | None = None):  # noqa: D401
        logger.info(_fmt(metrics))

    def _close():
        pass

    return _log, _close
